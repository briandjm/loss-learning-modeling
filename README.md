# Loss-Only vs. Reward-Based Reinforcement Learning in Macaque Decision-Making

This repository contains MATLAB code and data for a computational modeling project comparing behavior and model fits in macaques across two reinforcement contexts: **Reward Task** (gain/loss) and **Loss-Only Task** (all aversive outcomes).

The project was developed for a poster presentation at [Poster Day 2025], with analysis grounded in the modeling best practices of Wilson & Collins (2019).

---

## Code Overview

All scripts are currently in the main branch. Core components include:

- **Learning Curve Analysis**
  - `learning_curves.m`: plots behavioral performance across conditions
- **Model Comparison**
  - `basic_model_fit.m`: fits Random, WSLS, and Q-Learning models
  - `model_comparison_figs.m`: visualizes AIC, BIC, and log-likelihood
- **Parameter Estimation**
  - `batch_fit_analysis.m`: fits Q-learning Î± and Î² parameters across sessions
  - `parameter_boxplots.m`: generates boxplots of parameter distributions
- **Helper Functions**
  - `qlearn_loglik.m`: Q-learning likelihood function
  - `getcondition.m`: trial condition parser
- **Data**
  - `data_filtered.mat`: preprocessed behavioral data (TrialError=0, filtered)

---

## Models

- **Random**: Uniform choice
- **Win-Stay-Lose-Shift (WSLS)**: feedback-based heuristic
- **Q-Learning**: RL model using softmax decision rule (Î± = learning rate, Î² = inverse temperature)

Model fits were evaluated using **log-likelihood**, **AIC**, and **BIC**.

---

## Poster Figures

All major figures (learning curves, model comparison, parameter distributions) can be generated by running the main `.m` scripts. See inline comments for guidance.

---

## Reference

Wilson, R. C., & Collins, A. G. E. (2019).  
*Ten simple rules for the computational modeling of behavioral data.* *eLife*, 8, e49547. https://doi.org/10.7554/eLife.49547

---

## Acknowledgments

Thanks to Bruno Averbeck, and the NIH NIMH Laboratory of Neuropsychology.

---

## ðŸ”— License

MIT License â€“ feel free to reuse, adapt, and cite.

